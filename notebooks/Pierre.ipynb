{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to reproduce and explore the methodologies presented in the article \"Optimal Bayesian estimation of Gaussian mixtures with growing number of components\" by Ilsang Ohn and Lizhen Lin. The focus is on Bayesian estimation of finite Gaussian mixture models (GMMs) where the number of components is unknown and allowed to grow with the sample size.\n",
    "\n",
    "Gaussian mixture models are fundamental tools in statistical modeling and data analysis, used to represent a wide range of complex data distributions. However, estimating the parameters of GMMs, especially when the number of components is unknown and potentially large, poses significant challenges. Traditional methods may suffer from overfitting or underfitting, leading to inaccurate estimates and poor generalization.\n",
    "\n",
    "In this notebook, we aim to implement and compare various Bayesian estimation methods for finite Gaussian mixture models, including sample size-dependent priors and advanced computational algorithms. Investigate the theoretical foundations of these methods, focusing on optimal posterior contraction rates and their implications for model estimation. Evaluate the performance of these methods through simulation studies under different scenarios, including cases with well-separated components, overlapping components, weak components, and mixtures with a growing number of components. Apply the developed methods to real-world datasets, such as the Galaxy dataset and the Old Faithful geyser dataset, to demonstrate their practical utility and interpret the results. Explore methods for estimating the number of components in mixture models, leveraging Bayesian posterior inference and model selection criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Theoretical Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we provide a comprehensive overview of the theoretical foundations underlying the Bayesian estimation of finite Gaussian mixture models (GMMs) with a growing number of components. We introduce key concepts, notations, and mathematical results that are essential for understanding the methodologies implemented in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1 Finite Gaussian Mixture Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Definition\n",
    "\n",
    "A **finite Gaussian mixture model** represents a probability distribution as a convex combination of a finite number of Gaussian (normal) distributions. Formally, a Gaussian mixture model can be expressed as:\n",
    "\n",
    "$$\n",
    "p_{\\nu * \\Phi}(x) = \\int_{\\theta \\in \\Theta} \\varphi(x - \\theta) \\, \\nu(d\\theta),\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $x \\in \\mathbb{R}$ is the observed data.\n",
    "- $\\Theta \\subseteq \\mathbb{R}$ is the parameter space for the component means.\n",
    "- $\\varphi(x - \\theta)$ is the probability density function (pdf) of the normal distribution with mean $\\theta$ and variance $1$ (standard normal shifted by $\\theta$).\n",
    "- $\\nu$ is the **mixing distribution**, a probability measure on $\\Theta$.\n",
    "- $\\nu * \\Phi$ denotes the convolution of $\\nu$ with the standard normal distribution $\\Phi$.\n",
    "\n",
    "When $\\nu$ is a discrete measure with finite support, the mixture model becomes finite:\n",
    "\n",
    "$$\n",
    "\\nu = \\sum_{j=1}^k w_j \\delta_{\\theta_j},\n",
    "$$\n",
    "\n",
    "so that the mixture density simplifies to:\n",
    "\n",
    "$$\n",
    "p_{\\nu * \\Phi}(x) = \\sum_{j=1}^k w_j \\varphi(x - \\theta_j),\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $k \\in \\mathbb{N}$ is the number of components.\n",
    "- $w = (w_1, \\dots, w_k)$ are the **mixing weights**, satisfying $w_j \\geq 0$ and $\\sum_{j=1}^k w_j = 1$.\n",
    "- $\\theta = (\\theta_1, \\dots, \\theta_k) \\in \\Theta^k$ are the **component means**.\n",
    "- $\\delta_{\\theta_j}$ is the Dirac delta measure at $\\theta_j$.\n",
    "\n",
    "### Notations and Definitions\n",
    "\n",
    "- **Mixing Distribution Space**: Let $\\mathcal{M}(\\Theta)$ denote the set of all probability measures on $\\Theta$. Specifically, when $\\Theta = [-L, L]$ for some $L > 0$, we write $\\mathcal{M}([-L, L])$.\n",
    "- **Finite Mixtures**: Define $\\mathcal{M}_k \\subset \\mathcal{M}([-L, L])$ as the subset of mixing distributions that are discrete with at most $k$ atoms.\n",
    "- **Data Generation**: Assume we observe $X_1, X_2, \\dots, X_n$ independent and identically distributed (i.i.d.) samples from the mixture distribution $p_{\\nu * \\Phi}$.\n",
    "- **First-Order Wasserstein Distance**: For probability measures $\\mu$ and $\\nu$ on $\\mathbb{R}$, the first-order Wasserstein distance is defined as:\n",
    "\n",
    "  $$\n",
    "  W_1(\\mu, \\nu) = \\inf_{\\gamma \\in \\Gamma(\\mu, \\nu)} \\int_{\\mathbb{R} \\times \\mathbb{R}} |x - y| \\, d\\gamma(x, y),\n",
    "  $$\n",
    "\n",
    "  where $\\Gamma(\\mu, \\nu)$ is the set of all couplings of $\\mu$ and $\\nu$.\n",
    "\n",
    "### Identifiability\n",
    "\n",
    "- **Strong Identifiability**: A mixture model is said to be strongly identifiable if different mixing distributions correspond to different mixture densities.\n",
    "- **Implications**: For the Gaussian location mixture model with known variance, the model is identifiable under mild conditions. This property is crucial for consistent estimation of the mixing distribution.\n",
    "\n",
    "### Estimation Goals\n",
    "\n",
    "- **Estimating the Mixing Distribution**: Our primary goal is to estimate $\\nu$ based on the observed data.\n",
    "- **Estimating the Number of Components**: We also aim to estimate $k$, the true number of mixture components.\n",
    "- **Performance Metric**: We evaluate estimation accuracy using the Wasserstein distance $W_1(\\nu, \\hat{\\nu})$ between the true and estimated mixing distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2.2 Sample Size-Dependent Priors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Motivation\n",
    "\n",
    "In Bayesian inference for mixture models, the choice of prior distribution significantly influences the posterior estimates, especially for the number of components $ k $. A key challenge is to avoid overestimating $ k $ as the sample size $ n $ increases. To address this, we employ **sample size-dependent priors** that penalize models with a large number of components more heavily as $ n $ grows.\n",
    "\n",
    "### Hierarchical Prior Structure\n",
    "\n",
    "Our prior distribution $ \\Pi $ on the mixing distribution $ \\nu $ is constructed hierarchically:\n",
    "\n",
    "1. **Prior on the Number of Components $ k $**: $ \\Pi(k) $.\n",
    "2. **Prior on the Mixing Weights $ w $ Given $ k $**: $ \\Pi(w \\mid k) $.\n",
    "3. **Prior on the Component Means $ \\theta $ Given $ k $**: $ \\Pi(\\theta \\mid k) $.\n",
    "\n",
    "### Assumptions on the Prior\n",
    "\n",
    "We impose the following assumptions (denoted as **(P1)**, **(P2)**, and **(P3)**) on the prior distributions:\n",
    "\n",
    "#### (P1) Prior on the Number of Components\n",
    "\n",
    "There exist constants $ A > 0 $, $ c_1 > 0 $, and $ \\bar{k}_n = o\\left( \\dfrac{\\log n}{\\log \\log n} \\right) $ such that for all $ k \\leq \\bar{k}_n $:\n",
    "\n",
    "$$\n",
    "\\Pi(k) \\geq c_1 e^{-A k \\log n}.\n",
    "$$\n",
    "\n",
    "This assumption ensures that the prior probability of larger $ k $ decreases exponentially, preventing overestimation as $ n $ increases.\n",
    "\n",
    "#### (P2) Prior on the Mixing Weights\n",
    "\n",
    "There exist constants $ \\kappa_0 \\in (0, 1) $ and $ c_2 > 0 $ such that for any $ k \\in \\mathbb{N} $ and any $ w = (w_1, \\dots, w_k) $ satisfying $ w_j \\geq \\kappa_0 $ for all $ j $:\n",
    "\n",
    "$$\n",
    "\\Pi(w \\mid k) \\geq c_2.\n",
    "$$\n",
    "\n",
    "This ensures that the prior assigns positive probability to weight vectors where each component has a minimum weight, avoiding degenerate cases.\n",
    "\n",
    "#### (P3) Prior on the Component Means\n",
    "\n",
    "There exists a constant $ c_3 > 0 $ such that for any $ k \\in \\mathbb{N} $ and any $ \\theta = (\\theta_1, \\dots, \\theta_k) \\in [-L, L]^k $:\n",
    "\n",
    "$$\n",
    "\\Pi(\\theta \\mid k) \\geq c_3^k.\n",
    "$$\n",
    "\n",
    "This implies that the prior on $ \\theta $ is at least as diffuse as a uniform distribution over $ [-L, L]^k $.\n",
    "\n",
    "### Examples of Priors Satisfying the Assumptions\n",
    "\n",
    "#### Example 1: Mixture of Finite Mixtures (MFM) Prior\n",
    "\n",
    "- **Prior on $ k $**: A geometric distribution with success probability $ p_n = 1 - a e^{-A \\bar{k}_n \\log n} $ satisfies (P1).\n",
    "- **Prior on $ w $**: A Dirichlet distribution $ \\text{Dir}(\\kappa, \\dots, \\kappa) $ with $ \\kappa \\in (\\kappa_0, 1) $ satisfies (P2).\n",
    "- **Prior on $ \\theta $**: A uniform distribution over $ [-L, L]^k $ satisfies (P3).\n",
    "\n",
    "#### Example 2: Spike-and-Slab Prior\n",
    "\n",
    "- **Over-fitted Mixture**: Consider an over-fitted model with $ \\bar{k}_n $ components.\n",
    "- **Prior on Weights**: Assign a spike at zero with probability $ 1 - p_n $ and a slab (e.g., Gamma distribution) with probability $ p_n $.\n",
    "- **Induced Prior on $ k $**: The number of non-zero weights follows a Binomial distribution, satisfying (P1).\n",
    "\n",
    "### Benefits of Sample Size-Dependent Priors\n",
    "\n",
    "- **Control Model Complexity**: By penalizing larger $ k $, we prevent the posterior from favoring overly complex models.\n",
    "- **Optimal Estimation**: Enables the posterior to concentrate around the true mixing distribution at optimal rates.\n",
    "- **Avoid Overfitting**: Reduces the risk of overestimating the number of components due to random fluctuations in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.3 Posterior Contraction Rates and Optimality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Posterior Contraction Rate\n",
    "\n",
    "The **posterior contraction rate** quantifies how quickly the posterior distribution concentrates around the true parameter value as the sample size $ n $ increases. For estimating the mixing distribution $ \\nu $, we are interested in the rate $ \\epsilon_n $ such that:\n",
    "\n",
    "$$\n",
    "E_{P_{\\nu * \\Phi}} \\left[ \\Pi\\left( \\nu : W_1(\\nu, \\nu_0) \\geq M \\epsilon_n \\mid X_1^n \\right) \\right] \\rightarrow 0 \\quad \\text{as } n \\rightarrow \\infty,\n",
    "$$\n",
    "\n",
    "for some constant $ M > 0 $, where:\n",
    "\n",
    "- $ \\nu_0 $ is the true mixing distribution.\n",
    "- $ X_1^n = (X_1, \\dots, X_n) $ are the observed data.\n",
    "- $ E_{P_{\\nu * \\Phi}} $ denotes expectation under the true data-generating process.\n",
    "\n",
    "### Main Theoretical Results\n",
    "\n",
    "The following are key theoretical results from the article:\n",
    "\n",
    "#### Theorem 2.1: Posterior Does Not Overestimate $ k $\n",
    "\n",
    "Under assumptions (P1), (P2), and (P3), and provided $ k \\leq \\bar{k}_n = o\\left( \\dfrac{\\log n}{\\log \\log n} \\right) $, we have:\n",
    "\n",
    "$$\n",
    "\\inf_{\\nu \\in \\mathcal{M}_k} P_{\\nu * \\Phi} \\left( \\Pi\\left( \\nu \\in \\mathcal{M}_k \\mid X_1^n \\right) \\rightarrow 1 \\right).\n",
    "$$\n",
    "\n",
    "**Interpretation**: The posterior probability concentrates on mixing distributions with at most $ k $ components, avoiding overestimation of $ k $.\n",
    "\n",
    "#### Theorem 2.2: Optimal Posterior Contraction Rate for $ \\nu $\n",
    "\n",
    "Under the same assumptions, the posterior contraction rate for estimating $ \\nu $ is:\n",
    "\n",
    "$$\n",
    "\\epsilon_n = \\left( \\dfrac{\\log^2 n}{n} \\right)^{\\frac{1}{4k - 2}}.\n",
    "$$\n",
    "\n",
    "Specifically,\n",
    "\n",
    "$$\n",
    "\\sup_{\\nu \\in \\mathcal{M}_k} P_{\\nu * \\Phi} \\left( \\Pi\\left( W_1(\\nu, \\nu_0) \\geq M \\epsilon_n \\mid X_1^n \\right) = o(1) \\right),\n",
    "$$\n",
    "\n",
    "for some constant $ M > 0 $.\n",
    "\n",
    "**Interpretation**: The posterior distribution of $ \\nu $ concentrates around the true $ \\nu $ at the optimal rate $ \\epsilon_n $.\n",
    "\n",
    "#### Theorem 2.3: Improved Convergence Under Separation\n",
    "\n",
    "If the true mixing distribution $ \\nu $ has well-separated components, the posterior contraction rate improves to:\n",
    "\n",
    "$$\n",
    "\\epsilon_n = \\left( \\dfrac{\\log^2 n}{n} \\right)^{\\frac{1}{4(k - k_0) + 2}},\n",
    "$$\n",
    "\n",
    "where $ k_0 \\leq k $ is the number of well-separated components.\n",
    "\n",
    "**Definition of Separation**:\n",
    "\n",
    "- **Definition 1**: An atomic distribution $ \\nu = \\sum_{j=1}^k w_j \\delta_{\\theta_j} $ is said to be **$ k_0 (\\gamma, \\omega) $-separated** if:\n",
    "    - There exists a partition $ S_1, \\dots, S_{k_0} $ of $ \\{1, \\dots, k\\} $ such that:\n",
    "        - $ |\\theta_j - \\theta_{j'}| \\geq \\gamma $ for any $ j \\in S_l $, $ j' \\in S_{l'} $, $ l \\neq l' $.\n",
    "        - $ \\sum_{j \\in S_l} w_j \\geq \\omega $ for each $ l $.\n",
    "\n",
    "**Implications**:\n",
    "\n",
    "- Under separation, we effectively have fewer components ($ k_0 $) to estimate, leading to faster convergence.\n",
    "- The rate depends on the number of closely clustered components ($ k - k_0 $).\n",
    "\n",
    "### Minimax Optimality\n",
    "\n",
    "The rate $ \\epsilon_n $ matches the **minimax lower bound** up to logarithmic factors, meaning that no estimator can achieve a substantially faster rate in the worst-case scenario.\n",
    "\n",
    "- **Minimax Rate**: The best possible rate that any estimator can achieve, considering the most challenging distributions within the model class.\n",
    "- **Consequence**: Our Bayesian estimator is near-optimal in terms of convergence rate.\n",
    "\n",
    "### Posterior Consistency for $ k $\n",
    "\n",
    "Under certain conditions (e.g., perfectly separated components with weights bounded away from zero), the posterior distribution is consistent for the true number of components $ k $:\n",
    "\n",
    "$$\n",
    "\\inf_{\\nu \\in \\mathcal{M}_{k, k, \\gamma, \\omega}} P_{\\nu * \\Phi} \\left( \\Pi\\left( \\nu \\in \\mathcal{M}_k \\setminus \\mathcal{M}_{k - 1} \\mid X_1^n \\right) \\rightarrow 1 \\right).\n",
    "$$\n",
    "\n",
    "Here, $ \\mathcal{M}_{k, k, \\gamma, \\omega} $ denotes the set of mixing distributions that are $ k $-separated with parameters $ \\gamma $ and $ \\omega $.\n",
    "\n",
    "### Extension to Higher-Order Mixtures\n",
    "\n",
    "For models where $ k $ grows with $ n $ (higher-order mixtures), the posterior contraction rate becomes:\n",
    "\n",
    "$$\n",
    "\\epsilon_n = \\dfrac{\\log \\log n}{\\log n}.\n",
    "$$\n",
    "\n",
    "This rate is still minimax optimal for such models.\n",
    "\n",
    "### Significance of the Results\n",
    "\n",
    "- **Control of Overfitting**: Sample size-dependent priors prevent the posterior from overestimating the number of components, even as $ n $ grows.\n",
    "- **Optimal Estimation**: The Bayesian estimator achieves the fastest possible convergence rates under the given model assumptions.\n",
    "- **Adaptive Estimation**: The method adapts to the true level of complexity (e.g., effective number of components due to separation).\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "- By designing appropriate priors, practitioners can ensure that their Bayesian mixture models perform optimally, without overfitting or underfitting.\n",
    "- The theoretical guarantees provide confidence in applying these methods to real-world data, where the true number of components may be unknown and potentially large.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion of Theoretical Background**\n",
    "\n",
    "In this section, we have established the foundational concepts and mathematical results necessary for implementing and understanding Bayesian estimation methods for Gaussian mixture models with a growing number of components. By utilizing sample size-dependent priors and carefully analyzing posterior contraction rates, we can achieve optimal estimation performance, balancing model complexity and data fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Implementation of Bayesian Estimation Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.1 Sample Size-Dependent Priors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2 Reversible Jump MCMC (RJMCMC) Sampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.3 EM Algorithm for Posterior Mode Estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.4 Denoised Method of Moments (DMM) Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.5 Dirichlet Process (DP) Mixture Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Simulation Studies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.1 Design of Simulation Scenarios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.2 Implementation of Simulation Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.3 Evaluation Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.4 Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Estimation of the Number of Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5.1 Bayesian Estimation Using Posterior Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5.2 Model Selection Criteria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5.3 Cross-Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5.4 Comparison of Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 6. Real Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.1 Galaxy Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.2 Old Faithful Geyser Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Discussion and Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.1 Summary of Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.2 Computational Considerations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.3 Methodological Insights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.4 Future Work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Appendices (Optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## A.1 Mathematical Derivations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## A.2 Additional Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## A.3 Code Snippets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## F.1 Reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## F.2 References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## F.3 Collaboration and Sharing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
