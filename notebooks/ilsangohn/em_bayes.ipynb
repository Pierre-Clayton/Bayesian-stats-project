{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1bf115e-c4ac-4338-9f77-35e48448f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from model_gm import ModelGM\n",
    "\n",
    "class EM():\n",
    "\n",
    "    def __init__(self, k, sigma=None, interval=[-5,5], tol=1e-3, max_iter=100, print_iter=False):\n",
    "\n",
    "        self.k = k\n",
    "        self.sigma = sigma\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.print_iter = print_iter\n",
    "        self.interval = interval\n",
    "\n",
    "    def estimate(self, samples, num_rd=5, x_range=None, s_range=None):\n",
    "        \"\"\"\n",
    "        estimate a model\n",
    "        best of num_rd random initial guesses\n",
    "        initial centers are uniform from x_range\n",
    "        initial sigma are uniform from s_range\n",
    "\n",
    "        Args:\n",
    "        num_rd(int): number of random initial guess, default 5\n",
    "        x_range [a,b]: initial guess range of centers, default [-1,1]\n",
    "        s_range [c,d]: initial guess range of sigmas, default [0.5,1.5]\n",
    "        \"\"\"\n",
    "        if x_range is None:\n",
    "            x_range = [-1, 1]\n",
    "        if s_range is None:\n",
    "            s_range = [0.5, 1.5]\n",
    "\n",
    "        ll_max = float('-inf')\n",
    "        for _ in range(num_rd):\n",
    "            w_init = np.random.dirichlet(np.ones(self.k))\n",
    "            x_init = np.random.uniform(x_range[0], x_range[1], self.k)\n",
    "            if self.sigma is None:\n",
    "                s_init = np.random.uniform(s_range[0], s_range[1])\n",
    "            else:\n",
    "                s_init = self.sigma\n",
    "            start = ModelGM(w=w_init, x=x_init, std=s_init)\n",
    "            model_cur, _, ll_cur = self.estimate_with_init(samples, start, detail=True)\n",
    "            if ll_cur > ll_max:\n",
    "                model = model_cur\n",
    "                ll_max = ll_cur\n",
    "        return model\n",
    "\n",
    "    def estimate_with_init(self, samples, init, detail=False):\n",
    "        \"\"\"\n",
    "        estimate a model from a given initial\n",
    "        Args:\n",
    "        init (modelGM): initial guess\n",
    "\n",
    "        Returns:\n",
    "        model(modelGM): estimated model\n",
    "        iterN(int): number of iterations\n",
    "        ll_cur(float): last log-likelihood\n",
    "        \"\"\"\n",
    "        # assert self.k == len(init.weights)\n",
    "        k_cur = len(init.weights)\n",
    "        num = len(samples)\n",
    "        samples = np.asarray(samples)\n",
    "\n",
    "        num_iter = 0\n",
    "        model = init\n",
    "        l_mat = np.exp(ll_mat(samples, model)) # shape (n,k)\n",
    "        ll_cur = np.sum(np.log(np.dot(l_mat, model.weights)))\n",
    "\n",
    "        while True:\n",
    "            ll_pre = ll_cur\n",
    "\n",
    "            labels = l_mat * model.weights # shape (n,k)\n",
    "            labels /= np.sum(labels, axis=1)[:, np.newaxis]\n",
    "\n",
    "            sum_labels = np.sum(labels, axis=0) # shape (k,)\n",
    "\n",
    "            num_iter += 1\n",
    "            model.weights = (sum_labels+1)/(num+self.k)\n",
    "            centers = np.dot(samples, labels)/sum_labels\n",
    "            centers[centers>self.interval[1]] = self.interval[1]\n",
    "            centers[centers<self.interval[0]] = self.interval[0]\n",
    "            model.centers = centers\n",
    "\n",
    "            if self.sigma is None:\n",
    "                # EM iteration of estimating the common variance\n",
    "                cross = model.centers**2-2*np.outer(samples, model.centers)\\\n",
    "                        +(samples**2)[:, np.newaxis]\n",
    "                sigma2 = np.sum(cross*labels)/num\n",
    "                model.sigma = np.ones(k_cur) * np.sqrt(sigma2)\n",
    "\n",
    "            l_mat = np.exp(ll_mat(samples, model))\n",
    "            ll_cur = np.sum(np.log(np.dot(l_mat, model.weights)))\n",
    "\n",
    "            if self.print_iter:\n",
    "                print(model.weights, model.centers, model.sigma)\n",
    "                print(ll_cur)\n",
    "\n",
    "            if num_iter > self.max_iter or ll_cur-ll_pre < self.tol:\n",
    "                break\n",
    "\n",
    "        if detail:\n",
    "            return model, num_iter, ll_cur\n",
    "        else:\n",
    "            return model\n",
    "\n",
    "def ll_mat(samples, model):\n",
    "    \"\"\"\n",
    "    log-likelihood of samples\n",
    "\n",
    "    Args:\n",
    "    samples: ndarray of length n\n",
    "    model: ModelGM instance of k components with common sigma\n",
    "\n",
    "    Returns:\n",
    "    matrix of log-likelihoods of shape (n,k)\n",
    "    \"\"\"\n",
    "    samples = np.asarray(samples)\n",
    "    precision = 1./(model.sigma**2) # inverse of variance (sigma^2)\n",
    "    ll0 = model.centers**2*precision - 2*np.outer(samples, model.centers*precision) \\\n",
    "          +np.outer(samples**2, precision)\n",
    "    return -0.5*(np.log(2*np.pi)+ll0)-np.log(model.sigma)\n",
    "\n",
    "\n",
    "def ll_sample(samples, model):\n",
    "    \"\"\"\n",
    "    Log-likelihood matrix of samples under the given GM model\n",
    "\n",
    "    Args:\n",
    "    samples: ndarray of length n\n",
    "    model: ModelGM instance of k components with common sigma\n",
    "\n",
    "    Return:\n",
    "    log-likelihood of all samples\n",
    "    \"\"\"\n",
    "    return np.sum(np.log(np.dot(np.exp(ll_mat(samples, model)), model.weights)))\n",
    "\n",
    "\n",
    "def em_msel(x, kmin, kmax, kexact, interval, lam1, lam2, kappa):\n",
    "    \"\"\"\n",
    "    Model selection for Bayesian mixtures\n",
    "\n",
    "    Args:\n",
    "    samples: ndarray of length n\n",
    "    kmin, kmax: minimum and maximum number of components\n",
    "    kexact: true number of compoents\n",
    "    inteval: range of centers\n",
    "    lam1, lam2: two choices for mean parameter of Poisson prior\n",
    "    kappa: hyperparameter for Dirichlet prior\n",
    "    \"\"\"\n",
    "    est_list = []\n",
    "    post_list1 = []\n",
    "    post_list2 = []\n",
    "    for k in range(kmin, kmax+1):\n",
    "        em = EM(k=k, sigma=1)\n",
    "        est = em.estimate(x)\n",
    "        est_list.append(est)\n",
    "        log_lkd_val = log_lkd(x, est.weights, est.centers)\n",
    "        post_list1.append(log_lkd_val+log_prior(est.weights, interval, lam1, kappa))\n",
    "        post_list2.append(log_lkd_val+log_prior(est.weights, interval, lam2, kappa))\n",
    "        \n",
    "    k_map1 = np.argmax(post_list1)\n",
    "    k_map2 = np.argmax(post_list2)\n",
    "    \n",
    "    return  est_list[kexact-kmin], est_list[kmax-kmin], est_list[k_map1], est_list[k_map2]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63004953-f727-4399-93b3-ca2f944942e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
