{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30319765-499b-4623-8135-eabbd2d04f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## bayes\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import dirichlet\n",
    "\n",
    "def log_lkd(x, w, theta):\n",
    "    \"\"\"\n",
    "    log-likelihood of samples\n",
    "\n",
    "    Args:\n",
    "    x: samples\n",
    "    w: weight vector\n",
    "    theta: centers\n",
    "    \"\"\"\n",
    "    k = len(w)\n",
    "    n = len(x)\n",
    "    lkd_mtx = np.ones((n,k))\n",
    "    for j in range(k):\n",
    "        lkd_mtx[:,j] = w[j]*norm.pdf(x, loc=theta[j])         \n",
    "    lkd = np.sum(lkd_mtx, axis=1)\n",
    "    return np.sum(np.log(lkd))\n",
    "\n",
    "def log_prior(w, interval, lam, kappa):\n",
    "    \"\"\"\n",
    "    log of prior density\n",
    "\n",
    "    Args:\n",
    "    w: weight vector\n",
    "    interval: range of centers\n",
    "    lam: mean parameter of  Poisson prior\n",
    "    kappa: hyperparameter for Dirichlet prior\n",
    "    \"\"\"\n",
    "    k = len(w)\n",
    "    k_p = poisson.logpmf(k-1, lam)\n",
    "    if k>1:\n",
    "        w_p = dirichlet.logpdf(w, kappa*np.ones(k))  \n",
    "    else:\n",
    "        w_p = 0\n",
    "    theta_p = -k*np.log(interval[1]-interval[0])\n",
    "    return k_p + w_p + theta_p + math.lgamma(k+1)\n",
    "\n",
    "\n",
    "def bayesGM(x, interval, lam, kappa, n_save, n_burn, n_thin):\n",
    "    \"\"\"\n",
    "    RJMCMC sampler for Bayesian mixture\n",
    "\n",
    "    Args:\n",
    "    x: samples\n",
    "    interval: range of centers\n",
    "    lam: mean parameter of  Poisson prior\n",
    "    kappa: hyperparameter for Dirichlet prior\n",
    "    n_save, n_burn, n_thin: numbers of saved, burn-in and thinned MCMC samples, resp.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_iter = n_burn + n_save*n_thin    \n",
    "    k_mcmc = []\n",
    "    theta_mcmc = []\n",
    "    w_mcmc = []  \n",
    "    \n",
    "    ## initialize\n",
    "    w = dirichlet.rvs(np.ones(6))[0]\n",
    "    theta = np.random.uniform(interval[0], interval[1], size=6) \n",
    "    oll = log_lkd(x, w, theta)\n",
    "    olp = log_prior(w, interval, lam, kappa)\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        \n",
    "        OK = True\n",
    "        move = np.random.choice([1,2,3,4])\n",
    "        k = len(w)\n",
    "        \n",
    "        if move==1: # add a component   \n",
    "            j = np.random.choice(np.arange(k))\n",
    "            w_s = np.random.uniform(0, w[j], size=1)[0]\n",
    "            w1 = np.append(w, w_s)\n",
    "            w1[j] = w[j] - w_s                     \n",
    "            theta_s = np.random.uniform(interval[0], interval[1], size=1)[0]\n",
    "            theta1 = np.append(theta, theta_s)\n",
    "            log_rho_d = -np.log(k)\n",
    "            log_rho_n = -np.log(k*(k+1))\n",
    "            log_q_d = -np.log(interval[1]-interval[0]) - np.log(w1[j])\n",
    "            log_q_n = 0\n",
    "        \n",
    "        if move==2: #delete a component     \n",
    "            if k > 1:\n",
    "                jh = np.random.choice(np.arange(k), 2, replace=False)\n",
    "                j = jh[0]\n",
    "                h = jh[1]\n",
    "                w1 = w.copy()\n",
    "                w1[j] = w[j] + w[h]                \n",
    "                w1 = np.delete(w1, h)                \n",
    "                theta1 = np.delete(theta, h) \n",
    "                log_rho_d = -np.log(k*(k-1))\n",
    "                log_rho_n = -np.log(k-1)\n",
    "                log_q_d = 0\n",
    "                log_q_n = -np.log(interval[1]-interval[0]) - np.log(w[j]+w[h])\n",
    "            else:\n",
    "                OK = False\n",
    "        \n",
    "        if move==3: # sample theta\n",
    "            j = np.random.choice(np.arange(k))\n",
    "            theta1 = theta.copy()\n",
    "            theta1[j] = np.random.uniform(interval[0], interval[1], size=1)[0]\n",
    "            w1 = w.copy()\n",
    "            log_rho_d = log_rho_n = log_q_d = log_q_n = 0        \n",
    "        \n",
    "        if move==4: #sample w\n",
    "            if k > 1:\n",
    "                jh = np.random.choice(np.arange(k), 2, replace=False)\n",
    "                j = jh[0]\n",
    "                h = jh[1]\n",
    "                w1 = w.copy()\n",
    "                w1[j] = np.random.uniform(0, w[j] + w[h], size=1)[0]\n",
    "                w1[h] = w[j] + w[h] - w1[j]\n",
    "                theta1 = theta.copy()\n",
    "                log_rho_d = log_rho_n = log_q_d = log_q_n = 0    \n",
    "            else:\n",
    "                OK = False          \n",
    "                \n",
    "        if OK:            \n",
    "            nll = log_lkd(x, w1, theta1)\n",
    "            nlp = log_prior(w1, interval, lam, kappa)            \n",
    "            MHR = nll + nlp - oll - olp + log_rho_n - log_rho_d + log_q_n - log_q_d \n",
    "            \n",
    "            if np.log(np.random.uniform(size=1))<MHR:\n",
    "                w = w1.copy()\n",
    "                theta = theta1.copy()\n",
    "                oll = nll.copy()\n",
    "                olp = nlp.copy()              \n",
    "                \n",
    "        if (t+1>n_burn) & ((t+1-n_burn)%n_thin==0):\n",
    "            theta_mcmc.append(theta)\n",
    "            w_mcmc.append(w)\n",
    "            k_mcmc.append(len(w))\n",
    "            \n",
    "    return theta_mcmc, w_mcmc, k_mcmc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73984413-625f-421d-916c-09a84ce09ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## discrete_rv\n",
    "\n",
    "\"\"\"\n",
    "module for common operations on discrete random variables (distributions)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def assert_shape_equal(x_var, y_var):\n",
    "    \"\"\"\n",
    "    Assert shape equal\n",
    "    \"\"\"\n",
    "    if x_var.shape != y_var.shape:\n",
    "        raise AssertionError('Shape mismatch!')\n",
    "\n",
    "\n",
    "class DiscreteRV:\n",
    "    \"\"\" class for 1-d finite discrete RV\n",
    "    \"\"\"\n",
    "    def __init__(self, w, x):\n",
    "        \"\"\"\n",
    "        weights: probabilites masses\n",
    "        atoms: atoms\n",
    "        \"\"\"\n",
    "        self.weights = np.asarray(w)\n",
    "        self.atoms = np.asarray(x)\n",
    "        assert_shape_equal(self.weights, self.atoms)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"atom: %s\\nwght: %s\" % (self.atoms, self.weights)\n",
    "\n",
    "    def moment(self, degree=1):\n",
    "        \"\"\" Compute the moments of the input RV up to the given degree (start from first degree)\n",
    "        Args:\n",
    "        degree: int\n",
    "        highest degree k\n",
    "\n",
    "        Returns:\n",
    "        array of moments from the first degree to degree k\n",
    "        \"\"\"\n",
    "        moments = np.zeros(degree)\n",
    "        monomial = np.ones(self.atoms.shape)\n",
    "\n",
    "        for i in range(degree):\n",
    "            monomial *= self.atoms\n",
    "            moments[i] = np.dot(self.weights, monomial)\n",
    "\n",
    "        return moments\n",
    "\n",
    "    def dist_w1(self, another_rv):\n",
    "        \"\"\"\n",
    "        Compute the W1 distance from another_rv\n",
    "        \"\"\"\n",
    "        return wass(self, another_rv)\n",
    "\n",
    "\n",
    "    def sample(self, num):\n",
    "        \"\"\" draw n iid sample from U\n",
    "        library: https://docs.scipy.org/doc/numpy/reference/routines.random.html\n",
    "        alternative way: scipy.stats.rv_discrete(name='custm', values=(xk, pk))\n",
    "        \"\"\"\n",
    "        ## for 1-d RV\n",
    "        return np.random.choice(self.atoms, size=num, replace=True, p=self.weights)\n",
    "\n",
    "        ## for U in higher dimensions\n",
    "        # return U.x[np.random.choice(U.x.shape[0],size=n, replace=True, p=U.p)]\n",
    "\n",
    "\n",
    "\n",
    "    def sample_noisy(self, num, sigma=1):\n",
    "        \"\"\" draw n iid samples from model U+sigma Z. Default sigma=1.\n",
    "        \"\"\"\n",
    "        return self.sample(num) + sigma*np.random.randn(num)\n",
    "\n",
    "def wass(u_rv, v_rv):\n",
    "    \"\"\" compute W1 distance between DiscreteRVs U and V\n",
    "    \"\"\"\n",
    "    if len(u_rv.atoms) == 0 or len(v_rv.atoms) == 0:\n",
    "        return 0.\n",
    "\n",
    "    x_u, p_u = zip(*sorted(zip(u_rv.atoms, u_rv.weights)))\n",
    "    x_v, p_v = zip(*sorted(zip(v_rv.atoms, v_rv.weights)))\n",
    "    l_u, l_v, diff_cdf, dist, pre = 0, 0, 0., 0., 0.\n",
    "    while l_u < len(x_u) or l_v < len(x_v):\n",
    "        if l_v == len(x_v) or (l_u < len(x_u) and x_v[l_v] > x_u[l_u]):\n",
    "            dist += abs(diff_cdf)*(x_u[l_u]-pre)\n",
    "            pre = x_u[l_u]\n",
    "            diff_cdf += p_u[l_u]\n",
    "            l_u += 1\n",
    "        else:\n",
    "            dist += abs(diff_cdf)*(x_v[l_v]-pre)\n",
    "            pre = x_v[l_v]\n",
    "            diff_cdf -= p_v[l_v]\n",
    "            l_v += 1\n",
    "\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "939cd27f-e98f-4d5f-85ff-4ceafc9a2fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dmm\n",
    "\n",
    "\"\"\"\n",
    "main module\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import moments as mm\n",
    "from model_gm import ModelGM\n",
    "\n",
    "    # input: samples\n",
    "    # output: estimated model\n",
    "    # provide some optional parameters\n",
    "\n",
    "\n",
    "    # known variance\n",
    "    ## given weighting matrix\n",
    "    ## no weighting matrix\n",
    "    ### 1. use identity weighting matrix\n",
    "    ### 2. consistent estimation of optimal weighting matrix\n",
    "    ### 3. re-estimate\n",
    "\n",
    "\n",
    "    # unknown variance\n",
    "class DMM():\n",
    "    \"\"\"\n",
    "    class for denoised method of moments\n",
    "    \"\"\"\n",
    "    def __init__(self, k, interval=None, sigma=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        sigma: float, default None\n",
    "        standard deviation. If sigma == None, will estimate sigma.\n",
    "\n",
    "        interval =[a,b]: floats, default [-10, 10]\n",
    "        represents the interval of means [a,b]\n",
    "\n",
    "        num_components: int, required\n",
    "        number of postulated components\n",
    "        \"\"\"\n",
    "        if interval is None:\n",
    "            interval = [-10, 10]\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.k = k\n",
    "        self.interval = interval\n",
    "\n",
    "        if self.sigma is None:\n",
    "            self.num_mom = 2*self.k\n",
    "        else:\n",
    "            self.num_mom = 2*self.k-1\n",
    "            self.transform = mm.hermite_transform_matrix(2*self.k-1, self.sigma)\n",
    "\n",
    "        # for online estimation\n",
    "        # if sigma is None, this stores list of (sample size, raw moments)\n",
    "        # if sigma is known, this store list of (sample size, Hermite moments, correlation)\n",
    "        if self.sigma is None:\n",
    "            self.online = [0, np.zeros((self.num_mom, 1))]\n",
    "        else:\n",
    "            self.online = [0, np.zeros((self.num_mom, 1)), np.zeros((self.num_mom, self.num_mom))]\n",
    "\n",
    "\n",
    "    def estimate(self, samples):\n",
    "        \"\"\"\n",
    "        estimate a model from given samples\n",
    "        use two-step estimate:\n",
    "        1.(a) prelimnary estimation with identity weight matrix\n",
    "          (b) estimation of optimal weight matrix (require all samples)\n",
    "        2.    reestimate parameters using esimated weight matrix\n",
    "\n",
    "        Args:\n",
    "        samples: array of float, required\n",
    "        samples collected\n",
    "\n",
    "        Returns:\n",
    "        an estimated ModelGM\n",
    "        \"\"\"\n",
    "        samples = np.asarray(samples)\n",
    "        m_raw = mm.empirical_moment(samples, self.num_mom)\n",
    "\n",
    "        if self.sigma is None:\n",
    "            m_raw = np.mean(m_raw, axis=1)\n",
    "            m_esti, var_esti = mm.deconvolve_unknown_variance(m_raw)\n",
    "            dmom_rv = mm.quadmom(m_esti[:2*self.k-1])\n",
    "            return ModelGM(w=dmom_rv.weights, x=dmom_rv.atoms, std=np.sqrt(var_esti))\n",
    "        else:\n",
    "            m_hermite = np.dot(self.transform[0], m_raw)+self.transform[1]\n",
    "            m_decon = np.mean(m_hermite, axis=1)\n",
    "            dmom_rv = self.estimate_from_moments(m_decon) # preliminary estimate\n",
    "            wmat = estimate_weight_matrix(m_hermite, dmom_rv)\n",
    "            dmom_rv = self.estimate_from_moments(m_decon, wmat) # second step estimate\n",
    "            # print(np.linalg.inv(wmat))\n",
    "            return ModelGM(w=dmom_rv.weights, x=dmom_rv.atoms, std=self.sigma)\n",
    "\n",
    "    def estimate_online(self, samples_new):\n",
    "        \"\"\"\n",
    "        update the estimate a model from more samples\n",
    "        only store a few moments and correlations\n",
    "\n",
    "        Args:\n",
    "        samples_new: array of floats\n",
    "        new samples\n",
    "\n",
    "        Returns:\n",
    "        an estimated ModelGM\n",
    "        \"\"\"\n",
    "        samples_new = np.asarray(samples_new)\n",
    "        m_new = mm.empirical_moment(samples_new, self.num_mom) # moments, shape (L,n)\n",
    "        n_new = len(samples_new)\n",
    "        n_total = self.online[0]+n_new\n",
    "\n",
    "        if self.sigma:\n",
    "            m_new = np.dot(self.transform[0], m_new)+self.transform[1]\n",
    "            cor_new = np.dot(m_new, m_new.T)/n_new\n",
    "            self.online[2] = self.online[2]*(self.online[0]/n_total)+cor_new*(n_new/n_total)\n",
    "\n",
    "        mom_new = np.mean(m_new, axis=1)[:, np.newaxis] # empirical moments, shape (L,1)\n",
    "        self.online[1] = self.online[1]*(self.online[0]/n_total)+mom_new*(n_new/n_total)\n",
    "        self.online[0] = n_total\n",
    "\n",
    "        if self.sigma is None:\n",
    "            m_esti, var_esti = mm.deconvolve_unknown_variance(self.online[1])\n",
    "            dmom_rv = mm.quadmom(m_esti[:2*self.k-1])\n",
    "            return ModelGM(w=dmom_rv.weights, x=dmom_rv.atoms, std=np.sqrt(var_esti))\n",
    "        else:\n",
    "            wmat = np.linalg.inv(self.online[2]-np.dot(self.online[1], self.online[1].T))\n",
    "            dmom_rv = self.estimate_from_moments(self.online[1].reshape(self.num_mom), wmat)\n",
    "            # print(np.linalg.inv(wmat))\n",
    "            return ModelGM(w=dmom_rv.weights, x=dmom_rv.atoms, std=self.sigma)\n",
    "\n",
    "    def estimate_with_wmat(self, samples, wmat=None):\n",
    "        \"\"\"\n",
    "        estimate a model from given samples using given weight matrix\n",
    "        model: X=U+sigma*Z\n",
    "        sigma must be given\n",
    "\n",
    "        Args:\n",
    "        samples: array of float, required\n",
    "        samples collected\n",
    "\n",
    "        wmat: array of shape (k,k)\n",
    "        weight matrix, default identity\n",
    "\n",
    "        Returns:\n",
    "        latent distribtuion\n",
    "        \"\"\"\n",
    "        assert self.sigma\n",
    "\n",
    "        m_latent = self.estimate_latent_moments(samples)\n",
    "        dmom_rv = self.estimate_from_moments(m_latent, wmat)\n",
    "        return ModelGM(w=dmom_rv.weights, x=dmom_rv.atoms, std=self.sigma)\n",
    "\n",
    "    def estimate_select(self, samples, threhold=1):\n",
    "        \"\"\"\n",
    "        estimate with selected number of components\n",
    "        \"\"\"\n",
    "        k_cur = min(self.select_num_comp(samples, threhold), self.k)\n",
    "        dmm_cur = DMM(k=k_cur, interval=self.interval, sigma=self.sigma)\n",
    "        return dmm_cur.estimate(samples)\n",
    "\n",
    "    def estimate_latent_moments(self, samples):\n",
    "        \"\"\"\n",
    "        estimate moments of latent distribution (deconvolution)\n",
    "        model: X=U+sigma*Z\n",
    "        sigma must be given\n",
    "\n",
    "        Args:\n",
    "        samples: array of length n\n",
    "\n",
    "        Return:\n",
    "        array of length 2k-1\n",
    "        estimated moments of U from degree 1 to 2k-1\n",
    "        \"\"\"\n",
    "        assert self.sigma\n",
    "\n",
    "        samples = np.asarray(samples)\n",
    "        m_raw = mm.empirical_moment(samples, self.num_mom)\n",
    "        m_raw = np.mean(m_raw, axis=1).reshape((self.num_mom, 1))\n",
    "        return ((np.dot(self.transform[0], m_raw)+self.transform[1])).reshape(self.num_mom)\n",
    "\n",
    "    def estimate_from_moments(self, moments, wmat=None):\n",
    "        \"\"\"\n",
    "        estimate a discrete random variable from moments estimate\n",
    "\n",
    "        Args:\n",
    "        moments: array of length 2k-1\n",
    "        estimated moments of U of degree 1 to 2k-1\n",
    "\n",
    "        wmat: matrix of shape (k, k)\n",
    "        weight matrix for moment projection, default identity matrix\n",
    "\n",
    "        Returns:\n",
    "        an estimated latent distribtuion on at most k points\n",
    "        \"\"\"\n",
    "        m_proj = mm.projection(moments, self.interval, wmat)\n",
    "        dmom_rv = mm.quadmom(m_proj, dettol=0)\n",
    "        return dmom_rv\n",
    "\n",
    "    def sample_moment_cov(self, samples):\n",
    "        \"\"\"\n",
    "        return the sample covariance matrix of moments estimates\n",
    "        \"\"\"\n",
    "        samples = np.asarray(samples)\n",
    "        mom = mm.empirical_moment(samples, self.num_mom) # moments, shape (L,n)\n",
    "        mean = np.mean(mom, axis=1)\n",
    "        num = len(samples)\n",
    "        cor = np.dot(mom, mom.T)/num - np.outer(mean, mean)\n",
    "        return cor\n",
    "\n",
    "    def select_num_comp(self, samples, threhold=1):\n",
    "        \"\"\"\n",
    "        select the number of components\n",
    "        according to sample variance of moments estimate\n",
    "        \"\"\"\n",
    "        samples = np.asarray(samples)\n",
    "        num = len(samples)\n",
    "\n",
    "        m_raw = np.ones(len(samples))\n",
    "        deg_cur = 0\n",
    "        while True:\n",
    "            deg_cur += 1\n",
    "            m_raw *= samples\n",
    "            var = np.mean(m_raw**2)-np.mean(m_raw)**2\n",
    "            if var > threhold*num:\n",
    "                break\n",
    "\n",
    "        # moments of degree 1 to (deg_cur-1) is accurate\n",
    "        if self.sigma:\n",
    "            return int(np.floor(deg_cur/2))\n",
    "        else:\n",
    "            return int(np.floor((deg_cur-1)/2))\n",
    "\n",
    "def estimate_weight_matrix(m_estimate, model):\n",
    "    \"\"\"\n",
    "    estimate weight matrix: inverse of the estimated covariance matrix\n",
    "    ref: [Bruce E. Hansen] Econometrics. Chapter 11.\n",
    "\n",
    "    Args:\n",
    "    m_estimate: matrix of size (k,n)\n",
    "    power of n samples from degree of 1 to k\n",
    "\n",
    "    model: discrete_rv\n",
    "\n",
    "    Return:\n",
    "    consistent estimation for the optimal weight matrix\n",
    "    \"\"\"\n",
    "    num_moments, num_samples = m_estimate.shape\n",
    "    mom_model = model.moment(num_moments).reshape((num_moments, 1))\n",
    "    m_cond = m_estimate - mom_model\n",
    "    m_cond_avg = np.mean(m_cond, axis=1).reshape((num_moments, 1))\n",
    "    m_cond_centd = m_cond - m_cond_avg\n",
    "    return np.linalg.inv(np.dot(m_cond_centd, m_cond_centd.T)/num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "644c9ff0-99e5-4c75-ba58-457528217321",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DP\n",
    "\n",
    "import numpy as np\n",
    "#from nn_model_fts import *\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "\n",
    "def DP(x, interval, kappa, n_save, n_burn, n_thin):\n",
    "    \n",
    "    n_iter = n_burn + n_save*n_thin    \n",
    "    k_mcmc = []\n",
    "    theta_mcmc = []\n",
    "    \n",
    "    n = len(x)\n",
    "    K_new=5\n",
    "    ## initialize\n",
    "    theta = np.random.uniform(interval[0], interval[1], size=6) \n",
    "    z = np.random.choice(np.arange(6)+1, size=n)\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        \n",
    "        for i in range(n):\n",
    "            T = np.max(z)\n",
    "            z_i = np.delete(z, i)\n",
    "            theta_new = np.random.uniform(interval[0], interval[1], size=K_new)             \n",
    "            \n",
    "            unnorm_prob = np.zeros(T+K_new)\n",
    "            for j in range(T+K_new):\n",
    "                if j < T:\n",
    "                    unnorm_prob[j] = np.sum(np.equal(z,j+1))*norm.pdf(x[i], loc=theta[j])   \n",
    "                else:\n",
    "                    unnorm_prob[j] = kappa*norm.pdf(x[i], loc=theta_new[j-T])/K_new      \n",
    "                    \n",
    "            prob =  unnorm_prob/np.sum(unnorm_prob)\n",
    "            z_new = np.random.choice(np.arange(T+K_new) + 1, p=prob)\n",
    "            \n",
    "            if np.sum(np.equal(z_i, z[i]))>0: \n",
    "                if z_new > T:\n",
    "                    z[i] = T + 1 \n",
    "                    theta = np.append(theta, theta_new[z_new-T-1])\n",
    "                else:\n",
    "                    z[i] = z_new \n",
    "                    \n",
    "            else: \n",
    "                if z_new > T:\n",
    "                    theta[z[i]-1] = theta_new[z_new-T-1]\n",
    "                else:\n",
    "                    if z_new !=z[i]:\n",
    "                        z_old = z[i]\n",
    "                        z[i] = z_new\n",
    "                        theta = np.delete(theta, z_old-1)                        \n",
    "                        z = z - (np.array(z)>z_old)                             \n",
    "            \n",
    "        T = np.max(z)    \n",
    "        for j in range(T):\n",
    "            x_j = x[np.equal(z, j+1)]\n",
    "            theta[j] = truncnorm.rvs(interval[0], interval[1], loc=np.mean(x_j), scale=1/np.sqrt(len(x_j)))      \n",
    "                \n",
    "        if (t+1>n_burn) & ((t+1-n_burn)%n_thin==0):\n",
    "            theta_mcmc.append(theta)\n",
    "            k_mcmc.append(len(np.unique(z)))\n",
    "            \n",
    "    return theta_mcmc,  k_mcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f45776-df4c-475c-8bfc-94382752174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## em_bayes\n",
    "\n",
    "import numpy as np\n",
    "from model_gm import ModelGM\n",
    "\n",
    "class EM():\n",
    "\n",
    "    def __init__(self, k, sigma=None, interval=[-5,5], tol=1e-3, max_iter=100, print_iter=False):\n",
    "\n",
    "        self.k = k\n",
    "        self.sigma = sigma\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.print_iter = print_iter\n",
    "        self.interval = interval\n",
    "\n",
    "    def estimate(self, samples, num_rd=5, x_range=None, s_range=None):\n",
    "        \"\"\"\n",
    "        estimate a model\n",
    "        best of num_rd random initial guesses\n",
    "        initial centers are uniform from x_range\n",
    "        initial sigma are uniform from s_range\n",
    "\n",
    "        Args:\n",
    "        num_rd(int): number of random initial guess, default 5\n",
    "        x_range [a,b]: initial guess range of centers, default [-1,1]\n",
    "        s_range [c,d]: initial guess range of sigmas, default [0.5,1.5]\n",
    "        \"\"\"\n",
    "        if x_range is None:\n",
    "            x_range = [-1, 1]\n",
    "        if s_range is None:\n",
    "            s_range = [0.5, 1.5]\n",
    "\n",
    "        ll_max = float('-inf')\n",
    "        for _ in range(num_rd):\n",
    "            w_init = np.random.dirichlet(np.ones(self.k))\n",
    "            x_init = np.random.uniform(x_range[0], x_range[1], self.k)\n",
    "            if self.sigma is None:\n",
    "                s_init = np.random.uniform(s_range[0], s_range[1])\n",
    "            else:\n",
    "                s_init = self.sigma\n",
    "            start = ModelGM(w=w_init, x=x_init, std=s_init)\n",
    "            model_cur, _, ll_cur = self.estimate_with_init(samples, start, detail=True)\n",
    "            if ll_cur > ll_max:\n",
    "                model = model_cur\n",
    "                ll_max = ll_cur\n",
    "        return model\n",
    "\n",
    "    def estimate_with_init(self, samples, init, detail=False):\n",
    "        \"\"\"\n",
    "        estimate a model from a given initial\n",
    "        Args:\n",
    "        init (modelGM): initial guess\n",
    "\n",
    "        Returns:\n",
    "        model(modelGM): estimated model\n",
    "        iterN(int): number of iterations\n",
    "        ll_cur(float): last log-likelihood\n",
    "        \"\"\"\n",
    "        # assert self.k == len(init.weights)\n",
    "        k_cur = len(init.weights)\n",
    "        num = len(samples)\n",
    "        samples = np.asarray(samples)\n",
    "\n",
    "        num_iter = 0\n",
    "        model = init\n",
    "        l_mat = np.exp(ll_mat(samples, model)) # shape (n,k)\n",
    "        ll_cur = np.sum(np.log(np.dot(l_mat, model.weights)))\n",
    "\n",
    "        while True:\n",
    "            ll_pre = ll_cur\n",
    "\n",
    "            labels = l_mat * model.weights # shape (n,k)\n",
    "            labels /= np.sum(labels, axis=1)[:, np.newaxis]\n",
    "\n",
    "            sum_labels = np.sum(labels, axis=0) # shape (k,)\n",
    "\n",
    "            num_iter += 1\n",
    "            model.weights = (sum_labels+1)/(num+self.k)\n",
    "            centers = np.dot(samples, labels)/sum_labels\n",
    "            centers[centers>self.interval[1]] = self.interval[1]\n",
    "            centers[centers<self.interval[0]] = self.interval[0]\n",
    "            model.centers = centers\n",
    "\n",
    "            if self.sigma is None:\n",
    "                # EM iteration of estimating the common variance\n",
    "                cross = model.centers**2-2*np.outer(samples, model.centers)\\\n",
    "                        +(samples**2)[:, np.newaxis]\n",
    "                sigma2 = np.sum(cross*labels)/num\n",
    "                model.sigma = np.ones(k_cur) * np.sqrt(sigma2)\n",
    "\n",
    "            l_mat = np.exp(ll_mat(samples, model))\n",
    "            ll_cur = np.sum(np.log(np.dot(l_mat, model.weights)))\n",
    "\n",
    "            if self.print_iter:\n",
    "                print(model.weights, model.centers, model.sigma)\n",
    "                print(ll_cur)\n",
    "\n",
    "            if num_iter > self.max_iter or ll_cur-ll_pre < self.tol:\n",
    "                break\n",
    "\n",
    "        if detail:\n",
    "            return model, num_iter, ll_cur\n",
    "        else:\n",
    "            return model\n",
    "\n",
    "def ll_mat(samples, model):\n",
    "    \"\"\"\n",
    "    log-likelihood of samples\n",
    "\n",
    "    Args:\n",
    "    samples: ndarray of length n\n",
    "    model: ModelGM instance of k components with common sigma\n",
    "\n",
    "    Returns:\n",
    "    matrix of log-likelihoods of shape (n,k)\n",
    "    \"\"\"\n",
    "    samples = np.asarray(samples)\n",
    "    precision = 1./(model.sigma**2) # inverse of variance (sigma^2)\n",
    "    ll0 = model.centers**2*precision - 2*np.outer(samples, model.centers*precision) \\\n",
    "          +np.outer(samples**2, precision)\n",
    "    return -0.5*(np.log(2*np.pi)+ll0)-np.log(model.sigma)\n",
    "\n",
    "\n",
    "def ll_sample(samples, model):\n",
    "    \"\"\"\n",
    "    Log-likelihood matrix of samples under the given GM model\n",
    "\n",
    "    Args:\n",
    "    samples: ndarray of length n\n",
    "    model: ModelGM instance of k components with common sigma\n",
    "\n",
    "    Return:\n",
    "    log-likelihood of all samples\n",
    "    \"\"\"\n",
    "    return np.sum(np.log(np.dot(np.exp(ll_mat(samples, model)), model.weights)))\n",
    "\n",
    "\n",
    "def em_msel(x, kmin, kmax, kexact, interval, lam1, lam2, kappa):\n",
    "    \"\"\"\n",
    "    Model selection for Bayesian mixtures\n",
    "\n",
    "    Args:\n",
    "    samples: ndarray of length n\n",
    "    kmin, kmax: minimum and maximum number of components\n",
    "    kexact: true number of compoents\n",
    "    inteval: range of centers\n",
    "    lam1, lam2: two choices for mean parameter of Poisson prior\n",
    "    kappa: hyperparameter for Dirichlet prior\n",
    "    \"\"\"\n",
    "    est_list = []\n",
    "    post_list1 = []\n",
    "    post_list2 = []\n",
    "    for k in range(kmin, kmax+1):\n",
    "        em = EM(k=k, sigma=1)\n",
    "        est = em.estimate(x)\n",
    "        est_list.append(est)\n",
    "        log_lkd_val = log_lkd(x, est.weights, est.centers)\n",
    "        post_list1.append(log_lkd_val+log_prior(est.weights, interval, lam1, kappa))\n",
    "        post_list2.append(log_lkd_val+log_prior(est.weights, interval, lam2, kappa))\n",
    "        \n",
    "    k_map1 = np.argmax(post_list1)\n",
    "    k_map2 = np.argmax(post_list2)\n",
    "    \n",
    "    return  est_list[kexact-kmin], est_list[kmax-kmin], est_list[k_map1], est_list[k_map2]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bd723f7-93fd-412d-b82b-22a916d47f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model_gm\n",
    "\n",
    "\"\"\"\n",
    "module for Gaussian mixture model\n",
    "\"\"\"\n",
    "from discrete_rv import DiscreteRV, assert_shape_equal\n",
    "from moments import hermite_transform_matrix, empirical_moment\n",
    "import numpy as np\n",
    "\n",
    "class ModelGM:\n",
    "    \"\"\" class for 1-d Gaussian mixture model\n",
    "    weights: ndarray of weights\n",
    "    centers: ndarray of means\n",
    "    sigma: standard deviations\n",
    "    \"\"\"\n",
    "    def __init__(self, w=1, x=0, std=1):\n",
    "        \"\"\" Initialize a GM model\n",
    "        w: arrary of weights\n",
    "        x: arrary of means\n",
    "        std: scalar or array of standard deviations. If scalar, components share the same std.\n",
    "        \"\"\"\n",
    "\n",
    "        self.weights = np.asarray(w)\n",
    "        self.centers = np.asarray(x)\n",
    "        assert_shape_equal(self.weights, self.centers)\n",
    "        # np.testing.assert_array_equal(self.p.shape, self.mu.shape)\n",
    "        \n",
    "        if np.isscalar(std):\n",
    "            self.sigma = std*np.ones(self.weights.shape)\n",
    "        else:\n",
    "            self.sigma = np.asarray(std)\n",
    "        assert_shape_equal(self.weights, self.sigma)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"atom: %s\\nwght: %s\\nsigm: %s\" % (self.centers, self.weights, self.sigma[0])\n",
    "\n",
    "    def moments_gm(self, degree):\n",
    "        \"\"\"\n",
    "        moments of GM model\n",
    "\n",
    "        Args:\n",
    "        degree: int\n",
    "        highest degree k\n",
    "\n",
    "        Returns:\n",
    "        moments of Gaussian mixture model from degree 1 to k\n",
    "        \"\"\"\n",
    "        mom = empirical_moment(self.centers/self.sigma, degree)\n",
    "        transform = hermite_transform_matrix(degree)\n",
    "        transform = (np.abs(transform[0]), np.abs(transform[1]))\n",
    "        mom = np.dot(transform[0], mom) + transform[1]\n",
    "        s_pow = empirical_moment(self.sigma, degree)\n",
    "        mom = s_pow*mom\n",
    "        return np.dot(mom, self.weights)\n",
    "\n",
    "    def mean_rv(self):\n",
    "        \"\"\"\n",
    "        discrete rv for the means\n",
    "        \"\"\"\n",
    "        return DiscreteRV(self.weights, self.centers)\n",
    "\n",
    "    def std_rv(self):\n",
    "        \"\"\"\n",
    "        discrete rv for the sigmas\n",
    "        \"\"\"\n",
    "        return DiscreteRV(self.weights, self.sigma)\n",
    "\n",
    "\n",
    "\n",
    "def sample_gm(model, num, seed):\n",
    "    \"\"\"\n",
    "    n random samples from Gaussian mixture model\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.choice(model.centers.shape[0], size=num, replace=True, p=model.weights)\n",
    "    x = model.centers[idx] + model.sigma[idx] * np.random.randn(num)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f215c802-c4ca-43ee-9118-2e459987fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## moments\n",
    "\n",
    "\"\"\"\n",
    "module for operations on moments\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import cvxpy\n",
    "from scipy.linalg import hankel\n",
    "from discrete_rv import DiscreteRV\n",
    "import warnings\n",
    "\n",
    "def empirical_moment(samples, degree):\n",
    "    \"\"\" Compute empirical moments of samples\n",
    "    Args:\n",
    "    samples: x=(x1...xn)\n",
    "    degree: L\n",
    "\n",
    "    Returns\n",
    "    matrix M of size L*n\n",
    "    each row is the moments of (x1...xn) (start from the first degree to degree L)\n",
    "    \"\"\"\n",
    "    m_raw = np.empty((degree, len(samples)))\n",
    "    m_raw[0, :] = samples\n",
    "    for i in range(1, degree):\n",
    "        m_raw[i, :] = m_raw[i-1, :] * samples\n",
    "\n",
    "    return m_raw\n",
    "\n",
    "def hermite_transform_matrix(degree, sigma=1):\n",
    "    \"\"\" Hermite transformation\n",
    "    Let x=(x,...,x^k)', then Ax+b=(g_1(x,sigma),...,g_k(x,sigma))\n",
    "    g_k(x,sigma)=sigma^k*H_k(x/sigma), g_k(x,1)=H_k(x), the usual Hermite polynomial\n",
    "\n",
    "    Args:\n",
    "    degree: int\n",
    "    highest degree k\n",
    "\n",
    "    Return:\n",
    "    tuple (A,b): A is a matrix of shape (k,k), b is a vector of shape (k,1)\n",
    "    \"\"\"\n",
    "    length = degree+1\n",
    "    var = sigma*sigma\n",
    "    mat = np.zeros((length, length))\n",
    "    if length > 0:\n",
    "        prepre = np.zeros(length)\n",
    "        prepre[0] = 1\n",
    "        mat[0, :] = prepre\n",
    "    if length > 1:\n",
    "        pre = np.zeros(length)\n",
    "        pre[1] = 1\n",
    "        mat[1, :] = pre\n",
    "    for k in range(2, length):\n",
    "        # recursion: H_{n+1}(x) = x * H_n(x) - n * H_{n-1}(x)\n",
    "        # => g_{n+1}(x,s) = x * g_n(x,s) - n * s^2 * g_{n-1}(x,s)\n",
    "        coeffs = np.roll(pre, 1) - prepre*(k-1)*var\n",
    "        mat[k, :] = coeffs\n",
    "        prepre = pre\n",
    "        pre = coeffs\n",
    "\n",
    "    return (mat[1:, 1:], mat[1:, 0].reshape((degree, 1)))\n",
    "\n",
    "\n",
    "def transform(mat, x_var):\n",
    "    \"\"\"\n",
    "    Compute a linear tranformation Ax+b\n",
    "\n",
    "    mat: Tuple (A,b)\n",
    "    A: matrix of shape (k,k)\n",
    "    b: matrix of shape (k,1)\n",
    "\n",
    "    x_var: variable x\n",
    "    array of length k\n",
    "\n",
    "    Returns:\n",
    "    array of length k, y=Ax+b\n",
    "    \"\"\"\n",
    "    k = len(mat)\n",
    "    x_var = x_var.reshape((k, 1))\n",
    "    y_var = np.dot(mat[0], x_var)+mat[1]\n",
    "    return y_var.reshape(k)\n",
    "\n",
    "\n",
    "\n",
    "def projection(moments, interval=None, wmat=None):\n",
    "    \"\"\" project to a valid moment sequence on interval [a,b]\n",
    "\n",
    "    Args:\n",
    "    moments: a sequence of estimated moments, starting from degree 1\n",
    "    interval [a,b]: range of distributions, default [-1,1]\n",
    "    wmat: weighting matrix, default identity matrix\n",
    "\n",
    "    Returns:\n",
    "    a sequence of valid moments on [a,b], starting from degree 1 (default [a,b]=[-1,1])\n",
    "    minimize (moments-x)' * wmat * (moments-x), subject to x is a valid moment sequence\n",
    "    \"\"\"\n",
    "    if interval is None:\n",
    "        interval = [-1, 1]\n",
    "\n",
    "    length = len(moments)\n",
    "    if length == 0:\n",
    "        return moments\n",
    "    if length == 1:\n",
    "        moments[0] = max(interval[0], min(interval[1], moments[0]))\n",
    "        return moments\n",
    "\n",
    "\n",
    "    # preliminary filtering of moments based on range\n",
    "    r_max = max(abs(interval[0]),abs(interval[1]))\n",
    "    m_max = 1\n",
    "    for i in range(len(moments)):\n",
    "        m_max *= r_max\n",
    "        if moments[i] > m_max:\n",
    "            moments[i] = m_max\n",
    "        elif moments[i] < -m_max:\n",
    "            moments[i] = -m_max    \n",
    "\n",
    "    # SDP for further projection\n",
    "    variables = cvxpy.Variable(length) # variables [m_1,m_2,...,m_n]\n",
    "    if wmat is None:\n",
    "        wmat = np.identity(length)\n",
    "    obj = cvxpy.Minimize(cvxpy.quad_form(moments-variables, wmat)) # objective function\n",
    "    # obj = cvxpy.Minimize(cvxpy.sum_squares(x - moments)) \n",
    "\n",
    "    # the following gives constraints\n",
    "    # Ref for PSD condition: [Lasserre 2009, Theorem 3.3 and 3.4]\n",
    "    if length % 2 == 1:\n",
    "        # odd case\n",
    "        k = int((length+1)/2)\n",
    "        h_mat = cvxpy.Variable((k, k+1))\n",
    "        constraints = [h_mat[:, 1:]-interval[0]*h_mat[:, :k]>>0,\n",
    "                       interval[1]*h_mat[:, :k]-h_mat[:, 1:]>>0]\n",
    "    else:\n",
    "        # even case\n",
    "        k = int(length/2)+1\n",
    "        h_mat = cvxpy.Variable((k, k))\n",
    "        constraints = [h_mat>>0,\n",
    "                       (interval[0]+interval[1])*h_mat[:k-1, 1:]-interval[0]*interval[1]*h_mat[:k-1, :k-1]-h_mat[1:, 1:]>>0]\n",
    "    num_row, num_col = h_mat.shape\n",
    "    for i in range(num_row):\n",
    "        for j in range(num_col):\n",
    "            if i == 0 and j == 0:\n",
    "                constraints.append(h_mat[0, 0] == 1)\n",
    "            else:\n",
    "                constraints.append(h_mat[i, j] == variables[i+j-1])\n",
    "\n",
    "    prob = cvxpy.Problem(obj, constraints)\n",
    "    try:\n",
    "        prob.solve(solver=cvxpy.CVXOPT)\n",
    "    except Exception as e:\n",
    "        warnings.warn(\"CVXOPT failed. Using SCS solver...\"+str(e))\n",
    "        prob.solve(solver=cvxpy.SCS)\n",
    "        # prob.solve()\n",
    "    # opt = prob.solve(solver=cvxpy.CVXOPT)\n",
    "\n",
    "    return np.asarray(variables.value).reshape(moments.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def quadmom(moments, dettol=0, inf=1e10):\n",
    "    \"\"\" compute quadrature from moments\n",
    "    ref: Gene Golub, John Welsch, Calculation of Gaussian Quadrature Rules\n",
    "\n",
    "    Args:\n",
    "    m: moments sequence\n",
    "    dettol: tolerant of singularity of moments sequence (quantified by determinant of moment matrix)\n",
    "    INF: infinity\n",
    "\n",
    "    Returns:\n",
    "    U: quadrature\n",
    "    \"\"\"\n",
    "\n",
    "    moments = np.asarray(moments)\n",
    "    # INF = float('inf')\n",
    "    inf = 1e10\n",
    "\n",
    "    if len(moments) % 2 == 1:\n",
    "        moments = np.append(moments, inf)\n",
    "    num = int(len(moments)/2)\n",
    "    moments = np.insert(moments, 0, 1)\n",
    "\n",
    "\n",
    "    h_mat = hankel(moments[:num+1:], moments[num::]) # Hankel matrix\n",
    "    for i in range(len(h_mat)):\n",
    "        # check positive definite and decide to use how many moments\n",
    "        if np.linalg.det(h_mat[0:i+1, 0:i+1]) <= dettol: # alternative: less than some threshold\n",
    "            h_mat = h_mat[0:i+1, 0:i+1]\n",
    "            h_mat[i, i] = inf\n",
    "            num = i\n",
    "            break\n",
    "    r_mat = np.transpose(np.linalg.cholesky(h_mat)) # upper triangular Cholesky factor\n",
    "\n",
    "    # Compute alpha and beta from r, using Golub and Welsch's formula.\n",
    "    alpha = np.zeros(num)\n",
    "    alpha[0] = r_mat[0][1] / r_mat[0][0]\n",
    "    for i in range(1, num):\n",
    "        alpha[i] = r_mat[i][i+1]/r_mat[i][i] - r_mat[i-1][i]/r_mat[i-1][i-1]\n",
    "\n",
    "    beta = np.zeros(num-1)\n",
    "    for i in range(num-1):\n",
    "        beta[i] = r_mat[i+1][i+1]/r_mat[i][i]\n",
    "\n",
    "    jacobi = np.diag(alpha, 0) + np.diag(beta, 1) + np.diag(beta, -1)\n",
    "\n",
    "    eigval, eigvec = np.linalg.eig(jacobi)\n",
    "\n",
    "    atoms = eigval\n",
    "    weights = moments[0] * np.power(eigvec[0], 2)\n",
    "\n",
    "    return DiscreteRV(w=weights, x=atoms)\n",
    "\n",
    "\n",
    "def deconvolve_unknown_variance(moments):\n",
    "    \"\"\" Deconvolution with unknown sigma, using Lindsay's estimator.\n",
    "    Fit moments with U+sigma Z. Estimate common sigma, and moments of U.\n",
    "\n",
    "    Args:\n",
    "    moments: array of float, length 2k\n",
    "    moments estimate of degree 1 to 2k\n",
    "\n",
    "    Returns:\n",
    "    Tuple of deconvolved moments and estimated variance (sigma^2)\n",
    "    \"\"\"\n",
    "\n",
    "    moments = np.insert(moments, 0, 1)\n",
    "\n",
    "    length = len(moments)\n",
    "    m_hermite = [0]*length\n",
    "    x_var = np.poly1d([1, 0]) # x = sigma^2\n",
    "\n",
    "    if length > 0:\n",
    "        prepre = np.zeros(moments.shape)\n",
    "        prepre[0]=1\n",
    "        m_hermite[0] = moments[0]\n",
    "    if length > 1:\n",
    "        pre = np.zeros(moments.shape)\n",
    "        pre[1]=1\n",
    "        m_hermite[1] = moments[1]\n",
    "    for k in range(2, length):\n",
    "        # recursion: H_{n+1}(x) = x * H_n(x) - n * H_{n-1}(x)\n",
    "        coeffs = np.roll(pre, 1) - prepre*(k-1)\n",
    "        for i in range(k+1):\n",
    "            m_hermite[k] += float(coeffs[i]*moments[i])*(x_var**(int((k-i)/2)))\n",
    "        prepre = pre\n",
    "        pre = coeffs\n",
    "\n",
    "    # Solve the first non-negative root\n",
    "    equation = det_mom(m_hermite)\n",
    "\n",
    "    root = equation.r\n",
    "    root = root[np.isreal(root)].real\n",
    "    root = root[root >= 0]\n",
    "    root.sort()\n",
    "    # print(root)\n",
    "    root0 = root[0]\n",
    "\n",
    "    for k in range(2, length):\n",
    "        m_hermite[k] = m_hermite[k](root0)\n",
    "\n",
    "    return (np.asarray(m_hermite[1:]), float(root0))\n",
    "\n",
    "\n",
    "\n",
    "def det_mom(moments):\n",
    "    \"\"\"\n",
    "    Compute determinant of moment matrix\n",
    "\n",
    "    Args:\n",
    "    moments: array of moments of\n",
    "\n",
    "    Return:\n",
    "    determinant of moment matrix\n",
    "    \"\"\"\n",
    "    return determinant(get_hankel(moments))\n",
    "\n",
    "\n",
    "def get_hankel(moments):\n",
    "    \"\"\"\n",
    "    Construct Hankel matrix from (m_0,...,m_{2k})\n",
    "\n",
    "    Args:\n",
    "    moments: array of floats of length 2k+1\n",
    "    moments of degrees from 0 to 2k\n",
    "\n",
    "    Return:\n",
    "    Hankel matrix from those moments of size (k+1,k+1)\n",
    "    \"\"\"\n",
    "    # length of m = 2k+1 = 2k_inc-1\n",
    "    k_inc = int((len(moments)+1)/2)\n",
    "    matrix = [[0]*k_inc for i in range(k_inc)]\n",
    "    for i in range(k_inc):\n",
    "        for j in range(k_inc):\n",
    "            matrix[i][j] = moments[i+j]\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def determinant(mat, rows=None, cols=None):\n",
    "    \"\"\"\n",
    "    Compute the determinant of a submatrix\n",
    "\n",
    "    Args:\n",
    "    mat: matrix, list of lists\n",
    "    input matrix\n",
    "\n",
    "    rows: list of int\n",
    "    selection of rows\n",
    "\n",
    "    cols: list of int, same size as rows\n",
    "    selection of columns\n",
    "\n",
    "    Returns:\n",
    "    determinant of submatrix mat[rows, cols]\n",
    "    \"\"\"\n",
    "    if rows is None:\n",
    "        num_rows, num_cols = len(mat), len(mat[0])\n",
    "        rows = list(range(num_rows))\n",
    "        cols = list(range(num_cols))\n",
    "\n",
    "    num_rows = len(rows)\n",
    "    num_cols = len(cols)\n",
    "    assert num_rows == num_cols\n",
    "\n",
    "    if num_rows == 1:\n",
    "        return mat[rows[0]][cols[0]]\n",
    "    # elif rows == 2:\n",
    "    #     return M[r[0]][c[0]]*M[r[1]][c[1]] - M[r[0]][c[1]]*M[r[1]][c[0]]\n",
    "    # elif rows == 3:\n",
    "    #     return (M[r[0]][c[0]]*M[r[1]][c[1]]*M[r[2]][c[2]]\n",
    "    #             + M[r[0]][c[1]]*M[r[1]][c[2]]*M[r[2]][c[0]]\n",
    "    #             + M[r[0]][c[2]]*M[r[1]][c[0]]*M[r[2]][c[1]])\n",
    "    #             - (M[r[0]][c[2]]*M[r[1]][c[1]]*M[r[2]][c[0]]\n",
    "    #             + M[r[0]][c[0]]*M[r[1]][c[2]]*M[r[2]][c[1]]\n",
    "    #             + M[r[0]][c[1]]*M[r[1]][c[0]]*M[r[2]][c[2]])\n",
    "    else:\n",
    "        det = 0\n",
    "        newr = rows[1:]\n",
    "        sign = 1\n",
    "        for k in range(num_cols):\n",
    "            newc = cols[:k] + cols[(k + 1):]\n",
    "            det += determinant(mat, newr, newc)*mat[rows[0]][cols[k]]*sign\n",
    "            sign *= -1\n",
    "        return det\n",
    "\n",
    "    #### Available methods for computing determinant using Sympy: bareis, berkowitz, det_LU\n",
    "    #### Method 1: berkowitz\n",
    "    # eq = sympy.Matrix(H).det(method='berkowitz').as_poly().expand()\n",
    "    #### Method 2: bareis\n",
    "    # eq = sympy.Matrix(H).det(method='bareis').as_poly()\n",
    "    # for i in eq.gens[1:]:\n",
    "    #     eq = eq.eval(i,1)\n",
    "    # return eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6db1e260-beb8-4984-a2f4-edcade32e5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cvxpy in /opt/conda/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: osqp>=0.6.2 in /opt/conda/lib/python3.12/site-packages (from cvxpy) (0.6.7.post3)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in /opt/conda/lib/python3.12/site-packages (from cvxpy) (0.9.0)\n",
      "Requirement already satisfied: scs>=3.2.4.post1 in /opt/conda/lib/python3.12/site-packages (from cvxpy) (3.2.7)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.12/site-packages (from cvxpy) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from cvxpy) (1.14.1)\n",
      "Requirement already satisfied: qdldl in /opt/conda/lib/python3.12/site-packages (from osqp>=0.6.2->cvxpy) (0.1.7.post4)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'log_lkd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m dmm \u001b[38;5;241m=\u001b[39m DMM(k\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmin([kexact, \u001b[38;5;241m4\u001b[39m]), interval\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m7\u001b[39m], sigma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)    \n\u001b[1;32m     61\u001b[0m dmm_est \u001b[38;5;241m=\u001b[39m dmm\u001b[38;5;241m.\u001b[39mestimate(x)\n\u001b[0;32m---> 62\u001b[0m map1, map2, bayes1, bayes2 \u001b[38;5;241m=\u001b[39m \u001b[43mem_msel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkappa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m wass_list[m,i,\u001b[38;5;241m0\u001b[39m,b] \u001b[38;5;241m=\u001b[39m wass(dmm_est\u001b[38;5;241m.\u001b[39mmean_rv(), model\u001b[38;5;241m.\u001b[39mmean_rv())\n\u001b[1;32m     65\u001b[0m wass_list[m,i,\u001b[38;5;241m1\u001b[39m,b] \u001b[38;5;241m=\u001b[39m wass(map1\u001b[38;5;241m.\u001b[39mmean_rv(), model\u001b[38;5;241m.\u001b[39mmean_rv())\n",
      "File \u001b[0;32m~/work/Bayesian-stats-project/notebooks/ilsangohn/em_bayes.py:156\u001b[0m, in \u001b[0;36mem_msel\u001b[0;34m(x, kmin, kmax, kexact, interval, lam1, lam2, kappa)\u001b[0m\n\u001b[1;32m    154\u001b[0m est \u001b[38;5;241m=\u001b[39m em\u001b[38;5;241m.\u001b[39mestimate(x)\n\u001b[1;32m    155\u001b[0m est_list\u001b[38;5;241m.\u001b[39mappend(est)\n\u001b[0;32m--> 156\u001b[0m log_lkd_val \u001b[38;5;241m=\u001b[39m \u001b[43mlog_lkd\u001b[49m(x, est\u001b[38;5;241m.\u001b[39mweights, est\u001b[38;5;241m.\u001b[39mcenters)\n\u001b[1;32m    157\u001b[0m post_list1\u001b[38;5;241m.\u001b[39mappend(log_lkd_val\u001b[38;5;241m+\u001b[39mlog_prior(est\u001b[38;5;241m.\u001b[39mweights, interval, lam1, kappa))\n\u001b[1;32m    158\u001b[0m post_list2\u001b[38;5;241m.\u001b[39mappend(log_lkd_val\u001b[38;5;241m+\u001b[39mlog_prior(est\u001b[38;5;241m.\u001b[39mweights, interval, lam2, kappa))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_lkd' is not defined"
     ]
    }
   ],
   "source": [
    "## test_mixing\n",
    "\n",
    "!pip install cvxpy\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from DP import *\n",
    "from bayes import *   \n",
    "from em_bayes import *\n",
    "from dmm import DMM\n",
    "from model_gm import ModelGM, sample_gm\n",
    "from discrete_rv import wass\n",
    "\n",
    "\n",
    "## Model define\n",
    "model_list = []\n",
    "model_list.append(ModelGM(w=[0.3, 0.2, 0.3, 0.2], x=[-3, -1, 1, 3], std=1))\n",
    "model_list.append(ModelGM(w=[0.25, 0.25, 0.25, 0.25], x=[-1.5, -1, 1, 3], std=1))\n",
    "model_list.append(ModelGM(w=[0.4, 0.1, 0.25, 0.25], x=[-3, -1, 1, 3], std=1))\n",
    "model_list.append(ModelGM(w=np.ones(7)/7, x=2*np.arange(7)-6, std=1))\n",
    "\n",
    "n_list = 250*(np.arange(8)+1)\n",
    "repeat = 50\n",
    "wass_list = np.zeros([4, len(n_list), 5, repeat])\n",
    "\n",
    "for m in range(len(model_list)):    \n",
    "    \n",
    "    model = model_list[m]\n",
    "    kexact = len(model.centers)\n",
    "    kmax = 2*kexact \n",
    "    kmin = 2\n",
    "    \n",
    "    for i, n in enumerate(n_list):         \n",
    "                \n",
    "        for b in range(repeat):\n",
    "            \n",
    "            x = sample_gm(model, n, seed=b)              \n",
    "            \n",
    "            dmm = DMM(k=np.min([kexact, 4]), interval=[-7,7], sigma=1)    \n",
    "            dmm_est = dmm.estimate(x)\n",
    "            map1, map2, bayes1, bayes2 = em_msel(x, kmin, kmax, kexact, interval=[-6,6], lam1=1, lam2=np.exp(-0.05*np.log(n)**2/np.log(np.log(n))), kappa=1)\n",
    "            \n",
    "            wass_list[m,i,0,b] = wass(dmm_est.mean_rv(), model.mean_rv())\n",
    "            wass_list[m,i,1,b] = wass(map1.mean_rv(), model.mean_rv())\n",
    "            wass_list[m,i,2,b] = wass(map2.mean_rv(), model.mean_rv())\n",
    "            wass_list[m,i,3,b] = wass(bayes1.mean_rv(), model.mean_rv())\n",
    "            wass_list[m,i,4,b] = wass(bayes2.mean_rv(), model.mean_rv())\n",
    "\n",
    "## PLOT\n",
    "\n",
    "wass_mean = wass_list.mean(axis=3)\n",
    "wass_std = wass_list.std(axis=3)\n",
    "\n",
    "markers=['^','o','s','D','*']\n",
    "name = ['DMM', 'MAP_exact', 'MAP_over', 'MFM_const', 'MFM_vary']\n",
    "    \n",
    "for m in range(len(model_list)):      \n",
    "    plt.figure()       \n",
    "    for i in range(5): \n",
    "        plt.plot(n_list, wass_mean[m,:,i], marker=markers[i], label=name[i])        \n",
    "        plt.fill_between(n_list, wass_mean[m,:,i]-wass_std[m,:,i], wass_mean[m,:,i]+wass_std[m,:,i], alpha=0.2)\n",
    "    if m==1:\n",
    "        plt.legend()\n",
    "    plt.xlabel(\"Sample size\")\n",
    "    plt.ylabel(\"Wasserstein 1\")  \n",
    "    plt.savefig(\"mixing\"+str(m+1)+\".png\")   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
